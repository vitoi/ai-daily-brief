# ä»£ç è§„èŒƒä¸æœ€ä½³å®è·µ

## ğŸ¯ æ¦‚è¿°

è‰¯å¥½çš„ä»£ç è§„èŒƒæ˜¯å›¢é˜Ÿåä½œå’Œé¡¹ç›®ç»´æŠ¤çš„åŸºç¡€ã€‚æœ¬æ–‡æ¡£å®šä¹‰äº†AI Daily Briefé¡¹ç›®çš„ç¼–ç æ ‡å‡†ã€å‘½åçº¦å®šã€æœ€ä½³å®è·µå’Œä»£ç å®¡æŸ¥å‡†åˆ™ã€‚

## ğŸ“ ä»£ç é£æ ¼

### Python ä»£ç é£æ ¼

é¡¹ç›®é‡‡ç”¨ [PEP 8](https://www.python.org/dev/peps/pep-0008/) ä½œä¸ºåŸºç¡€ä»£ç é£æ ¼æŒ‡å—ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹å·¥å…·è¿›è¡Œä»£ç æ ¼å¼åŒ–å’Œæ£€æŸ¥ï¼š

- **ä»£ç æ ¼å¼åŒ–**: [Black](https://black.readthedocs.io/)
- **ä»£ç æ£€æŸ¥**: [flake8](https://flake8.pycqa.org/)
- **ç±»å‹æ£€æŸ¥**: [mypy](https://mypy.readthedocs.io/)
- **å¯¼å…¥æ’åº**: [isort](https://pycqa.github.io/isort/)

#### Black é…ç½®
```ini
# pyproject.toml
[tool.black]
line-length = 88
target-version = ['py39']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''
```

#### flake8 é…ç½®
```ini
# .flake8
[flake8]
max-line-length = 88
extend-ignore = E203, W503
exclude =
    .git,
    __pycache__,
    build,
    dist,
    .venv,
    .tox,
    .eggs,
    *.egg
per-file-ignores =
    __init__.py:F401
```

### ä»£ç ç»“æ„è§„èŒƒ

#### æ–‡ä»¶ç»“æ„
```
src/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py                 # åº”ç”¨å…¥å£
â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†
â”œâ”€â”€ database.py            # æ•°æ®åº“ç›¸å…³
â”œâ”€â”€ models/                # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ news.py
â”‚   â””â”€â”€ brief.py
â”œâ”€â”€ collectors/            # æ•°æ®æ”¶é›†å™¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ rss_collector.py
â”‚   â””â”€â”€ api_collector.py
â”œâ”€â”€ processors/            # æ•°æ®å¤„ç†å™¨
â”œâ”€â”€ publishers/            # å‘å¸ƒå™¨
â””â”€â”€ utils/                 # å·¥å…·å‡½æ•°
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ helpers.py
    â””â”€â”€ validators.py
```

#### åŒ…å¯¼å…¥
```python
# æ ‡å‡†åº“å¯¼å…¥
import os
import sys
from typing import List, Dict, Optional

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import requests
from bs4 import BeautifulSoup
import sqlalchemy as sa

# æœ¬åœ°æ¨¡å—å¯¼å…¥
from .base import BaseCollector
from ..models.news import NewsItem
from ..utils.helpers import clean_text
```

## ğŸ“ å‘½åçº¦å®š

### ç±»å‘½å
```python
# æ­£ç¡®
class NewsCollector:
class RSSFeedParser:
class ContentProcessor:

# é”™è¯¯
class news_collector:  # åº”ä½¿ç”¨ PascalCase
class News_Collector:  # ä¸ä½¿ç”¨ä¸‹åˆ’çº¿
```

### å‡½æ•°å’Œæ–¹æ³•å‘½å
```python
# æ­£ç¡®
def collect_news_items():
def parse_rss_feed():
def validate_news_item():

# é”™è¯¯
def CollectNewsItems():  # åº”ä½¿ç”¨ snake_case
def parseRSSFeed():      # ä¸ä½¿ç”¨é©¼å³°å¼
```

### å˜é‡å‘½å
```python
# æ­£ç¡®
news_items = []
current_page = 1
max_retries = 3

# é”™è¯¯
newsItems = []      # ä¸ä½¿ç”¨é©¼å³°å¼
currentpage = 1     # å•è¯é—´åº”æœ‰ä¸‹åˆ’çº¿
MAX_RETRIES = 3     # å¸¸é‡é™¤å¤–
```

### å¸¸é‡å‘½å
```python
# æ­£ç¡®
MAX_RETRIES = 3
DEFAULT_TIMEOUT = 30
API_BASE_URL = "https://api.example.com"

# é”™è¯¯
maxRetries = 3      # åº”ä½¿ç”¨å¤§å†™å­—æ¯
default_timeout = 30 # å¸¸é‡åº”å¤§å†™
```

## ğŸ”§ ä»£ç è´¨é‡è¦æ±‚

### ç±»å‹æ³¨è§£

æ‰€æœ‰æ–°ä»£ç å¿…é¡»ä½¿ç”¨ç±»å‹æ³¨è§£ï¼š

```python
from typing import List, Dict, Optional, Union

def process_news_items(
    items: List[Dict[str, Union[str, int]]],
    max_items: Optional[int] = None
) -> List[Dict[str, str]]:
    """å¤„ç†æ–°é—»æ¡ç›®åˆ—è¡¨"""
    pass
```

### æ–‡æ¡£å­—ç¬¦ä¸²

æ‰€æœ‰å…¬å…±å‡½æ•°ã€ç±»å’Œæ–¹æ³•å¿…é¡»æœ‰æ–‡æ¡£å­—ç¬¦ä¸²ï¼š

```python
class NewsCollector:
    """æ–°é—»æ”¶é›†å™¨åŸºç±»

    è´Ÿè´£ä»å„ç§æ•°æ®æºæ”¶é›†æ–°é—»æ•°æ®ï¼Œå¹¶è¿›è¡Œåˆæ­¥å¤„ç†å’ŒéªŒè¯ã€‚

    Attributes:
        timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°
    """

    def __init__(self, timeout: int = 30, max_retries: int = 3):
        """åˆå§‹åŒ–æ”¶é›†å™¨

        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.timeout = timeout
        self.max_retries = max_retries

    def collect_news(self, source_url: str) -> List[Dict[str, str]]:
        """ä»æŒ‡å®šæºæ”¶é›†æ–°é—»

        Args:
            source_url: æ•°æ®æºURL

        Returns:
            æ–°é—»æ¡ç›®åˆ—è¡¨

        Raises:
            ConnectionError: ç½‘ç»œè¿æ¥å¤±è´¥
            ValidationError: æ•°æ®éªŒè¯å¤±è´¥
        """
        pass
```

### é”™è¯¯å¤„ç†

#### å¼‚å¸¸å¤„ç†åŸåˆ™
```python
# æ­£ç¡®ï¼šå…·ä½“çš„å¼‚å¸¸ç±»å‹
try:
    response = requests.get(url, timeout=self.timeout)
    response.raise_for_status()
except requests.Timeout:
    logger.warning(f"è¯·æ±‚è¶…æ—¶: {url}")
    raise ConnectionError(f"è¿æ¥è¶…æ—¶: {url}")
except requests.HTTPError as e:
    logger.error(f"HTTPé”™è¯¯: {e}")
    raise ValidationError(f"æ— æ•ˆå“åº”: {url}")
except Exception as e:
    logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
    raise

# é”™è¯¯ï¼šè¿‡äºå®½æ³›çš„å¼‚å¸¸å¤„ç†
try:
    # ä¸€äº›æ“ä½œ
    pass
except:
    pass  # ä¸è®°å½•é”™è¯¯ä¿¡æ¯
```

#### è‡ªå®šä¹‰å¼‚å¸¸
```python
# exceptions.py
class AIDailyBriefError(Exception):
    """AI Daily Brief åŸºç¡€å¼‚å¸¸"""
    pass

class CollectionError(AIDailyBriefError):
    """æ•°æ®æ”¶é›†å¼‚å¸¸"""
    pass

class ProcessingError(AIDailyBriefError):
    """æ•°æ®å¤„ç†å¼‚å¸¸"""
    pass

class PublishingError(AIDailyBriefError):
    """å‘å¸ƒå¼‚å¸¸"""
    pass
```

### æ—¥å¿—è®°å½•

#### æ—¥å¿—çº§åˆ«ä½¿ç”¨
```python
import logging

logger = logging.getLogger(__name__)

# DEBUG: è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
logger.debug("å¤„ç†æ–°é—»æ¡ç›®: %s", item_id)

# INFO: æ­£å¸¸æ“ä½œä¿¡æ¯
logger.info("æˆåŠŸæ”¶é›† %d æ¡æ–°é—»", count)

# WARNING: è­¦å‘Šä¿¡æ¯
logger.warning("æ•°æ®æº %s å“åº”ç¼“æ…¢", source_url)

# ERROR: é”™è¯¯ä¿¡æ¯
logger.error("æ”¶é›†å¤±è´¥: %s", str(e))

# CRITICAL: ä¸¥é‡é”™è¯¯
logger.critical("æ•°æ®åº“è¿æ¥ä¸¢å¤±")
```

#### ç»“æ„åŒ–æ—¥å¿—
```python
# æ¨èï¼šç»“æ„åŒ–æ—¥å¿—
logger.info("æ–°é—»æ”¶é›†å®Œæˆ", extra={
    "source": source_name,
    "count": item_count,
    "duration": duration,
    "status": "success"
})

# ä¸æ¨èï¼šå­—ç¬¦ä¸²æ ¼å¼åŒ–
logger.info(f"æ–°é—»æ”¶é›†å®Œæˆ: æº={source_name}, æ•°é‡={item_count}")
```

## ğŸ§ª æµ‹è¯•è§„èŒƒ

### æµ‹è¯•æ–‡ä»¶ç»“æ„
```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py              # pytest é…ç½®å’Œ fixtures
â”œâ”€â”€ unit/                   # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_collectors.py
â”‚   â”œâ”€â”€ test_processors.py
â”‚   â””â”€â”€ test_publishers.py
â”œâ”€â”€ integration/            # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ test_full_pipeline.py
â”‚   â””â”€â”€ test_external_apis.py
â””â”€â”€ fixtures/               # æµ‹è¯•æ•°æ®
    â”œâ”€â”€ sample_news.json
    â””â”€â”€ mock_responses.py
```

### æµ‹è¯•ç”¨ä¾‹ç¼–å†™
```python
import pytest
from unittest.mock import Mock, patch
from src.collectors.rss_collector import RSSCollector

class TestRSSCollector:

    @pytest.fixture
    def collector(self):
        return RSSCollector()

    @pytest.fixture
    def mock_feed_data(self):
        return {
            "entries": [
                {
                    "title": "AI News Title",
                    "link": "https://example.com/ai-news",
                    "summary": "AI news summary",
                    "published": "2025-01-17T10:00:00Z"
                }
            ]
        }

    def test_collect_success(self, collector, mock_feed_data):
        """æµ‹è¯•æˆåŠŸæ”¶é›†RSSæ•°æ®"""
        with patch('feedparser.parse') as mock_parse:
            mock_parse.return_value = mock_feed_data

            result = collector.collect("https://example.com/rss")

            assert len(result) == 1
            assert result[0]["title"] == "AI News Title"
            assert result[0]["url"] == "https://example.com/ai-news"

    def test_collect_network_error(self, collector):
        """æµ‹è¯•ç½‘ç»œé”™è¯¯å¤„ç†"""
        with patch('feedparser.parse', side_effect=Exception("Network error")):
            with pytest.raises(CollectionError):
                collector.collect("https://example.com/rss")

    @pytest.mark.asyncio
    async def test_collect_with_retry(self, collector):
        """æµ‹è¯•é‡è¯•æœºåˆ¶"""
        with patch.object(collector, '_fetch_feed') as mock_fetch:
            mock_fetch.side_effect = [Exception("Temp error"), Mock()]

            await collector.collect_with_retry("https://example.com/rss")

            assert mock_fetch.call_count == 2
```

### æµ‹è¯•è¦†ç›–ç‡è¦æ±‚
- **å•å…ƒæµ‹è¯•**: â‰¥ 80% è¦†ç›–ç‡
- **é›†æˆæµ‹è¯•**: æ ¸å¿ƒä¸šåŠ¡æµç¨‹å…¨è¦†ç›–
- **å›å½’æµ‹è¯•**: æ‰€æœ‰å·²çŸ¥bugçš„å›å½’æµ‹è¯•

## ğŸ”’ å®‰å…¨ç¼–ç å®è·µ

### è¾“å…¥éªŒè¯
```python
from pydantic import BaseModel, validator
import re

class NewsSource(BaseModel):
    """æ–°é—»æºé…ç½®æ¨¡å‹"""
    name: str
    url: str
    type: str

    @validator('url')
    def validate_url(cls, v):
        """éªŒè¯URLæ ¼å¼"""
        url_pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)

        if not url_pattern.match(v):
            raise ValueError('Invalid URL format')
        return v

    @validator('type')
    def validate_type(cls, v):
        """éªŒè¯ç±»å‹"""
        allowed_types = ['rss', 'api', 'scrape']
        if v not in allowed_types:
            raise ValueError(f'Type must be one of: {allowed_types}')
        return v
```

### æ•æ„Ÿæ•°æ®å¤„ç†
```python
import os
from cryptography.fernet import Fernet

class SecretManager:
    """æ•æ„Ÿæ•°æ®ç®¡ç†å™¨"""

    def __init__(self):
        key = os.getenv('ENCRYPTION_KEY')
        if not key:
            raise ValueError("ENCRYPTION_KEY environment variable not set")
        self.cipher = Fernet(key.encode())

    def encrypt(self, data: str) -> str:
        """åŠ å¯†æ•°æ®"""
        return self.cipher.encrypt(data.encode()).decode()

    def decrypt(self, encrypted_data: str) -> str:
        """è§£å¯†æ•°æ®"""
        return self.cipher.decrypt(encrypted_data.encode()).decode()
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### å¼‚æ­¥ç¼–ç¨‹
```python
import asyncio
import aiohttp
from typing import List, Dict

class AsyncNewsCollector:
    """å¼‚æ­¥æ–°é—»æ”¶é›†å™¨"""

    async def collect_multiple_sources(self, sources: List[Dict]) -> List[Dict]:
        """å¹¶å‘æ”¶é›†å¤šä¸ªæ•°æ®æº"""

        async def collect_single_source(source):
            async with aiohttp.ClientSession() as session:
                return await self._collect_from_source(session, source)

        # é™åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…è¿‡è½½
        semaphore = asyncio.Semaphore(10)

        async def limited_collect(source):
            async with semaphore:
                return await collect_single_source(source)

        tasks = [limited_collect(source) for source in sources]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœï¼Œè¿‡æ»¤å¼‚å¸¸
        valid_results = []
        for result in results:
            if not isinstance(result, Exception):
                valid_results.extend(result)

        return valid_results
```

### ç¼“å­˜ç­–ç•¥
```python
from functools import lru_cache
import redis
import json

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self):
        self.redis = redis.Redis(host='localhost', port=6379, db=0)

    @lru_cache(maxsize=1000)
    def get_cached_news(self, source_url: str, ttl: int = 3600) -> List[Dict]:
        """è·å–ç¼“å­˜çš„æ–°é—»æ•°æ®"""
        cache_key = f"news:{source_url}"

        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜ï¼ˆLRUï¼‰
        cached = self.get_cache(cache_key)
        if cached:
            return json.loads(cached)

        # ä»Redisè·å–
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached.decode())

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æºè·å–
        news_data = self._fetch_from_source(source_url)

        # è®¾ç½®ç¼“å­˜
        self.redis.setex(cache_key, ttl, json.dumps(news_data))

        return news_data
```

## ğŸ“‹ ä»£ç å®¡æŸ¥æ¸…å•

### åŠŸèƒ½å®Œæ•´æ€§
- [ ] ä»£ç å®ç°é¢„æœŸçš„åŠŸèƒ½
- [ ] é”™è¯¯å¤„ç†å®Œå–„
- [ ] è¾¹ç•Œæ¡ä»¶è€ƒè™‘å……åˆ†
- [ ] æ€§èƒ½æ»¡è¶³è¦æ±‚

### ä»£ç è´¨é‡
- [ ] éµå¾ªä»£ç è§„èŒƒ
- [ ] æœ‰å®Œæ•´çš„ç±»å‹æ³¨è§£
- [ ] æœ‰è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²
- [ ] é€šè¿‡æ‰€æœ‰lintingæ£€æŸ¥

### æµ‹è¯•è¦†ç›–
- [ ] æœ‰å¯¹åº”çš„å•å…ƒæµ‹è¯•
- [ ] æµ‹è¯•è¦†ç›–è¾¹ç•Œæ¡ä»¶
- [ ] æµ‹è¯•è¦†ç›–é”™è¯¯åœºæ™¯
- [ ] æµ‹è¯•ä»£ç ç¬¦åˆè§„èŒƒ

### å®‰å…¨æ£€æŸ¥
- [ ] æ— æ•æ„Ÿä¿¡æ¯æ³„éœ²
- [ ] è¾“å…¥éªŒè¯å®Œå–„
- [ ] SQLæ³¨å…¥é˜²æŠ¤
- [ ] XSSé˜²æŠ¤

### æ€§èƒ½è€ƒè™‘
- [ ] æ— æ˜æ˜¾æ€§èƒ½é—®é¢˜
- [ ] åˆç†ä½¿ç”¨ç¼“å­˜
- [ ] å¼‚æ­¥å¤„ç†åˆé€‚
- [ ] èµ„æºä½¿ç”¨åˆç†

## ğŸ“š å‚è€ƒèµ„æ–™

- [PEP 8 - Pythonä»£ç é£æ ¼æŒ‡å—](https://www.python.org/dev/peps/pep-0008/)
- [Google Python é£æ ¼æŒ‡å—](https://google.github.io/styleguide/pyguide.html)
- [The Hitchhiker's Guide to Python](https://docs.python-guide.org/)
- [Effective Python](https://effectivepython.com/)

---

*æœ¬æ–‡æ¡£ç‰ˆæœ¬: v1.0 | æœ€åæ›´æ–°: 2025-01-17*
